{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from skbio.stats.ordination import OrdinationResults\n",
    "\n",
    "# Für später\n",
    "# from qiime2 import Metadata\n",
    "# from qiime2 import Artifact\n",
    "# from qiime2.plugins import emperor\n",
    "# from qiime2.plugins import diversity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Dataframe erstellen mit metadata.tsv datei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1: Einlesen des Metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/home/sven/Desktop/bsc_env/qp-platemapper/platemapper/tests/data/Moeller_SANS1/meta_plate.tsv' , sep='\\t')\n",
    "#Path ist noch lokal, work in progress\n",
    "df2 = df1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Falls Metadata file einen anderen namen als well_id haben sollte dann namen finden und umbenennen, anschließend checken ob well_id teil der indizes ist.\n",
    "okay also problem, sobald die metadata file bereits den spaltennamen \"well_id\" enthält will die conversion nicht mehr funktionieren. Auch der untere Code bekommt es (noch) nicht hin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ja\n"
     ]
    }
   ],
   "source": [
    "werte = [\"A1\", \"A2\"] # Das hier muss noch unbedingt mit regex gemacht werden für den wahrscheinlichen fall dass auf der \n",
    "#plate die ersten beiden wells nicht benutzt werden.\n",
    "\n",
    "\n",
    "# Passende Spalte finden und umbenennen\n",
    "if \"well_id\" not in df2.columns:\n",
    "    for spalte in df2.columns:\n",
    "        if all(wert in df2[spalte].values for wert in werte):\n",
    "            print(f\"beide Werte in spalte: {spalte}\")\n",
    "else:\n",
    "    1\n",
    "   \n",
    "df2 = df2.rename(columns={spalte: \"well_id\"}) \n",
    "# Andernfalls eine variable mit den wellids erstellen und damit weiterarbeiten\n",
    "\n",
    "# testing: nachschauen ob es geklappt hat\n",
    "if \"well_id\" in df2.columns:\n",
    "    print(\"ja\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faxen (also wirklich nur faxen, es gibt eine so viel bessere idee und die werde ich dann auch machen allerdings funktioniert das hier (leider gottes) und es erlaubt mir die weiteren schritte zu gehen.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2[\"well_id\"] = df2[\"well_id\"].str.replace(\"A\", \"8, \", regex=False)\n",
    "df2[\"well_id\"] = df2[\"well_id\"].str.replace(\"B\", \"7, \", regex=False)\n",
    "df2[\"well_id\"] = df2[\"well_id\"].str.replace(\"C\", \"6, \", regex=False)\n",
    "df2[\"well_id\"] = df2[\"well_id\"].str.replace(\"D\", \"5, \", regex=False)\n",
    "df2[\"well_id\"] = df2[\"well_id\"].str.replace(\"E\", \"4, \", regex=False)\n",
    "df2[\"well_id\"] = df2[\"well_id\"].str.replace(\"F\", \"3, \", regex=False)\n",
    "df2[\"well_id\"] = df2[\"well_id\"].str.replace(\"G\", \"2, \", regex=False)\n",
    "df2[\"well_id\"] = df2[\"well_id\"].str.replace(\"H\", \"1, \", regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3: Das große Metadata file verkleinern in die für die Ordination interessanten Informationen(sample_name, well_id) herausfiltern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "time_point\n",
      "                  sample_name well_id\n",
      "144              14908.dlr145    4, 3\n",
      "145  14908.dlrnegcontrolK5Y8C   1, 12\n",
      "146  14908.dlrposcontrolK5Y8C   1, 11\n",
      "147  14908.dlrnegcontrolK65YY   1, 12\n",
      "148  14908.dlrposcontrolK65YY   1, 11\n"
     ]
    }
   ],
   "source": [
    "tempSamples = pd.concat([\n",
    "    df2[\"sample_name\"],\n",
    "    df2[\"well_id\"]],\n",
    "    axis =1 \n",
    "    )\n",
    "\n",
    "# Sehr wichtig: Es kann passieren dass in sample_name mehr rows \n",
    "# existieren als well_id werte, dann packt der code fälschlicherweise NaN values auf die row.\n",
    "# Hiermit versucht man dem ein bisschen entgegenzuwirken und so far funktioniert es\n",
    "x = 0\n",
    "for val in tempSamples[\"well_id\"].values:\n",
    "    x += 1\n",
    "    if type(val) is not str:\n",
    "        print(type(val)) # NaN ist n float\n",
    "        print(tempSamples.sample_name.values[x-1])\n",
    "        tempSamples = tempSamples.drop(x - 1)\n",
    "\n",
    "# testing: worked?\n",
    "print(tempSamples.tail(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ab hier, Chatgpt war die inspiration, nach kurzem googlen hab ich herausgefunden wie ich das für mich umschreiben muss. \n",
    "\n",
    "1.4: Die zusammengesetzten Zahlen der well_id trennen und in zwei spalten (Axis1, Axis2) trennen. Zum ende hin die nummern als float ausgeben lassen da es mit str vermutlich noch weniger funktioniert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Axis1  Axis2\n",
      "sample_name              \n",
      "14908.dlr1     1.0    8.0\n",
      "14908.dlr2     2.0    8.0\n",
      "14908.dlr3     3.0    8.0\n",
      "14908.dlr4     4.0    8.0\n",
      "14908.dlr5     5.0    8.0\n",
      "14908.dlr6     6.0    8.0\n",
      "14908.dlr7     7.0    8.0\n"
     ]
    }
   ],
   "source": [
    "#split\n",
    "tempSamples[[\"Axis1\", \"Axis2\"]] = tempSamples[\"well_id\"].str.split(',', n=1, expand=True)\n",
    "\n",
    "# dabei schon die Achsen tauschen, da:\n",
    "#     x -------->\n",
    "#         0 1 2 3 ...\n",
    "# y   a \n",
    "# |   b\n",
    "# |   c\n",
    "# |\n",
    "# Im Originalzustand enthält Axis1 die a,b,c... werte und Axis2 die 0-12 werte, die ordination datei nimmt als erste achse halt x\n",
    "# Ich brauche noch einen mechanismus welcher bei bspw. 1A als wellid die achsen nicht dreht\n",
    "\n",
    "#\"Drehen\" der beiden Achsen\n",
    "# datentypen fest deklarieren, Ordination kann nicht mit str arbeiten, muss/kann float sein\n",
    "temp = tempSamples[\"Axis1\"]\n",
    "tempSamples[\"Axis1\"] = tempSamples[\"Axis2\"].astype(float)\n",
    "tempSamples[\"Axis2\"] = temp.astype(float)\n",
    "\n",
    "#finalen dataframe erstellen aus welchem auch die ordination am ende entsteht\n",
    "samples = pd.DataFrame(\n",
    "    data=tempSamples[[\"Axis1\", \"Axis2\"]].values,\n",
    "    index=tempSamples[\"sample_name\"],\n",
    "    columns=[\"Axis1\", \"Axis2\"]\n",
    ")\n",
    "#test\n",
    "print(samples.head(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1: Eigenvalues und proportion_explained. Einfach fantasiewerte einfügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals = pd.Series([0.13, 0.37])\n",
    "proportion_explained = pd.Series([0.5758, 0.4242])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2: Erstellung der Ordination und export als ordination.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordination = OrdinationResults(\n",
    "    \"Manual\",\n",
    "    \"Parsed well ID ordination\",\n",
    "    eigvals,\n",
    "    samples,\n",
    "    proportion_explained = proportion_explained\n",
    "    # fun fact wenn man proportion_explained im selben stil wie samples & eigvals schreibt sprengt das komplett die Ordiantion\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ordination.txt\", \"w\") as f:\n",
    "    \n",
    "    ordination.write(f, format=\"ordination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nächster Teil: mit Python automatisch einen Path und Ordner für das alles erstellen und dann darüber automatisch die .qzv zu erstellen.\n",
    "Probleme die ich immernoch habe:    \n",
    "- Der Code kann nur eine plate erstellen(falls mehr koordinaten als plätze auf wells, und dann auch noch gleich, dann bomba)\n",
    "    - vielleicht dem Programm sagen, sobald mehr als 96 wellids da sind dann pack die in einen dateframe und geh weitere 96 schritte AUßER wir haben etabliert es handelt sich um eine 384x plate\n",
    "    - ODER: bei mehreren gleichen well_ids nach irgendeinem Alleinstellungsmerkmal(krank,gesund) filtern und danach mehrere\n",
    "    plates erzeugen.\n",
    "- 384x plates sind noch nicht möglich (probably easiest to do)\n",
    "    - Vielleicht irgendwie es schaffen, dem code zu sagen wie viele wells in einem Metafile stecken, demnach in mehrere Dataframes unterteilen\n",
    "    - in etwa so: for vals in df2[\"Axis1\"] if df2[\"Axis1\"].values >=13 && df2[\"Axis2\"].values >=9 dann 384 well plate\n",
    "- Qiime2 Integration in den Code, skip the Terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiime2-amplicon-2024.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
